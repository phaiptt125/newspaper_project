{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Continuous Bag of Words Model\n",
    "\n",
    "Online supplementary material to \"The Evolving U.S. Occupational Structure\" by Enghin Atalay, Phai Phongthiengtham, Sebastian Sotelo and Daniel Tannenbaum.\n",
    "\n",
    "* [Most recent version of the paper](\n",
    "http://ssc.wisc.edu/~eatalay/skills.pdf)\n",
    "\n",
    "* [Project data library](http://ssc.wisc.edu/~eatalay/occupation_data.html) \n",
    "\n",
    "* [GitHub repository](https://github.com/phaiptt125/newspaper_project)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IPython notebook demonstrates how we map between occupational characteristics to words/phrases from newspaper text using the Continuous Bag of Words Model (CBOW). \n",
    "\n",
    "* See [here](http://ssc.wisc.edu/~eatalay/apst/apst_mapping.pdf) for more examples.\n",
    "* See project data library for full results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Due to copyright restrictions, we are not authorized to publish a large body of newspaper text. </b>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import platform\n",
    "import collections\n",
    "import shutil\n",
    "\n",
    "import pandas\n",
    "import math\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Word2Vec, keyedvectors \n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, we construct our model by taking as our text corpora all of the text from job ads which appeared in our cleaned newspaper data, plus the raw text from job ads which were posted on-line in two months: January 2012 and January 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare newspaper text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For newspaper text data, we:\n",
    "\n",
    "1. Retrieve document metadata, remove markup from the newspaper text, and to perform an initial spell-check of the text (see [here](https://github.com/phaiptt125/newspaper_project/blob/master/data_cleaning/initial_cleaning.ipynb)). \n",
    "2. Exclude non-job ad pages (see [here](https://github.com/phaiptt125/newspaper_project/blob/master/data_cleaning/LDA.ipynb)).\n",
    "3. Transform unstructured newspaper text into spreadsheet data (see [here](https://github.com/phaiptt125/newspaper_project/blob/master/data_cleaning/structured_data.ipynb)).\n",
    "4. Delete all non alphabetic character, e.g., numbers and punctuations.\n",
    "5. Convert all character to lowercase. \n",
    "\n",
    "Example below demonstrates how to perform step 4 and 5 in a very short snippet of Display Ad page 226, from the January 14, 1979 Boston Globe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- newspaper text ---\n",
      "manage its Primary Care Programs including 24-hour Emergency Room Primary Care program\n",
      "\n",
      "--- transformed text ---\n",
      "manage its primary care programs including hour emergency room primary care program\n"
     ]
    }
   ],
   "source": [
    "text = \"manage its Primary Care Programs including 24-hour Emergency Room Primary Care program\"\n",
    "\n",
    "print('--- newspaper text ---')\n",
    "print(text)\n",
    "print('')\n",
    "print('--- transformed text ---')\n",
    "print(re.sub('[^a-z ]','',text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare online job posting text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Economic Modeling Specialists International (EMSI) provided us with online postings data in a processed format and relatively clean form: see [here](https://github.com/phaiptt125/online_job_posting/blob/master/data_cleaning/initial_cleaning.ipynb).\n",
    "\n",
    "For the purpose of this project, we use online postings data to:\n",
    "1. Enrich the sample of text usuage when constructing the Continuous Bag of Words model\n",
    "2. Retrieve a mapping between job title and ONET-SOC code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filename of the combined ads ~ 15 GB \n",
    "text_data_filename = 'ad_combined.txt'\n",
    "\n",
    "# construct CBOW model\n",
    "dim_model = 300\n",
    "model = Word2Vec(LineSentence(open(text_data_filename)), \n",
    "                 size=dim_model, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 workers=multiprocessing.cpu_count())\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# define output filename for CBOW model\n",
    "cbow_filename = 'cbow.model'\n",
    "\n",
    "# save model into file\n",
    "model.save(cbow_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(cbow_filename)\n",
    "word_all = model.wv # set of all words in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar_words(phrase,model,dim_model):\n",
    "    # This function compute similar words given a word or phrase.\n",
    "    # If the input is just one word, this function is the same as gensim built-in function: model.most_similar\n",
    "    \n",
    "    # phrase : input for word or phrases to look for. For a phrase with multiple words, add \"_\" in between.\n",
    "    # model : constructed CBOW model\n",
    "    # dim_model : dimension of the model, i.e., length of a vector of each word \n",
    "    \n",
    "    tokens = [w for w in re.split('_',phrase) if w in word_all] \n",
    "    # split input to tokens, ignoring words that are not in the model  \n",
    "    \n",
    "    vector_by_word = np.zeros((len(tokens),dim_model)) # initialize a matrix \n",
    "    \n",
    "    for i in range(0,len(tokens)):\n",
    "        word = tokens[i] # loop for each word\n",
    "        vector_this_word = model[word] # get a vector representation\n",
    "        vector_by_word[i,:] = vector_this_word # record the vector\n",
    "    \n",
    "    vector_this_phrase = sum(vector_by_word) \n",
    "    # sum over words to get a vector representation of the whole phrase\n",
    "    \n",
    "    most_similar_words = model.similar_by_vector(vector_this_phrase, topn=100, restrict_vocab=None)\n",
    "    # find 100 most similar words\n",
    "    \n",
    "    most_similar_words = [w for w in most_similar_words if not w[0] == phrase]\n",
    "    # take out the output word that is identical to the input word\n",
    "    \n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity score of any pair of words/phrases is defined to be a cosine of the two vectors representing those pair of words/phrases. Higher cosine similarity score means the two words/phrases tend to appear around similar context.  \n",
    "\n",
    "The function *find_similar_words* above returns a set of similar words, ordered by cosine similarity score, and their corresponding cosine similarity score. For example, the ten most similar words to \"creative\" are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('imaginative', 0.6997416615486145),\n",
       " ('versatile', 0.6824457049369812),\n",
       " ('creature', 0.591433584690094),\n",
       " ('innovative', 0.5758161544799805),\n",
       " ('resourceful', 0.5575118660926819),\n",
       " ('creallve', 0.5550633668899536),\n",
       " ('restive', 0.5526227951049805),\n",
       " ('dynamic', 0.5416233539581299),\n",
       " ('clever', 0.5349052548408508),\n",
       " ('pragmatic', 0.5299020409584045)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words = find_similar_words('creative',model,dim_model)\n",
    "most_similar_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the ten most similar words to \"bookkeeping\" are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bkkp', 0.6903467178344727),\n",
       " ('beekeeping', 0.6871334314346313),\n",
       " ('stenography', 0.672173023223877),\n",
       " ('bkkpng', 0.6181079745292664),\n",
       " ('bkkpg', 0.6175851821899414),\n",
       " ('bookkpg', 0.5925684571266174),\n",
       " ('dkkpg', 0.5809350609779358),\n",
       " ('bkkping', 0.5768048167228699),\n",
       " ('clerical', 0.5741672515869141),\n",
       " ('payroll', 0.5619226098060608)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words = find_similar_words('bookkeeping',model,dim_model)\n",
    "most_similar_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strength of the Continuous Bag of Words (CBOW) Model is twofold. First, the model provide context-based synonyms which allows us to keep track of relevant words even if their usage may differ over time. We provide one example in the main paper: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For instance, even though “creative” and “innovative” largely refer to the same occupational skill, it is possible that their relative usage among potential employers may differ within the sample period. This is indeed the case: Use of the word “innovative” has increased more quickly than “creative” over the sample period. To the extent that our ad hoc classification included only one of these two words, we would be mis-characterizing trends in the ONET skill of “Thinking Creatively.” The advantage of the continuous bag of words model is that it will identify that “creative” and “innovative” mean the same thing because they appear in similar contexts within job ads. Hence, even if employers start using “innovative” as opposed to “creative” part way through our sample, we will be able to consistently measure trends in “Thinking Creatively” throughout the entire period.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second advantage of the CBOW model is to identify common abbrevations and transcription errors. The word \"bookkeeping\", for instance, was offen mistranscribed into \"beekeeping\" due to the imperfection of the Optical Character Recognition (OCR) algorithm. Moreover, our CBOW model also reveals common abbrevations that employers offen used such as \"bkkp\" and \"bkkpng\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
